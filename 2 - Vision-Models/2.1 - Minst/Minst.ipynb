{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ~ PoC AI Pool 2025 ~\n",
    "- ## Day 3: Deep Learning\n",
    "    - ### Module 2: Convolutional Neural Network\n",
    "-----------\n",
    "\n",
    "## Minst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well done, you've arrived here ! You now understand key concepts of neural networks and how they are trained, but you haven't really created one yet...\n",
    "Don't worry this task will guide you in recreating a neural network trained to detect any handwritten digit on a 28 by 28 pixel image !\n",
    "\n",
    "Your will start by setup the dataset, your model and at the end, play with it ! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Just import the necessary libraries\n",
    "\n",
    "import time\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "#For the model don't forget\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part - 1 Prepare the data \n",
    "\n",
    "before actually create a neural network we need to preparate our data that we will fit to your model,\n",
    "\n",
    "remember ***THE MOST important in machine learning is the quality of the data*** and not really the model....\n",
    "\n",
    "your goal here is to specify how we want the data, this can be process by initialise a data and transform it in a [tensor](https://pytorch.org/vision/main/generated/torchvision.transforms.ToTensor.html) and normalize it if you want. you can check the doc of transform [here](https://pytorch.org/vision/0.9/transforms.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Len train dataset : 60000\n",
      "Len test  dataset : 10000\n"
     ]
    }
   ],
   "source": [
    "#TODO: define the transforms compose\n",
    "transform = transforms.ToTensor()\n",
    "\n",
    "train_set = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "eval_set = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "print(f\"Len train dataset : {len(train_set)}\")\n",
    "print(f\"Len test  dataset : {len(eval_set)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will say why whe created two dataset ? \n",
    "\n",
    "It's because one will be for the training of the model and the other for evaluate this one by passing data he never seen, to see if the model didn't overfit the data.\n",
    "\n",
    "To understand what's inside this code you can try below to visualise some of the examples !\n",
    "\n",
    "***Don't hesitate to change the NUMBER_OF_ELEMENTS enum to see mutliples examples or no***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels : 3; 7; 3; 8;\n"
     ]
    }
   ],
   "source": [
    "# Visualisation of some element of the dataset you can change the number if you want\n",
    "NUMBER_OF_ELEMENTS = 4\n",
    "\n",
    "def imshow(img):\n",
    "    # img = img * 0.5 + 0.5  # Denormalisation if you have normalised the data\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)), cmap='gray')\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "train_loader_vis = torch.utils.data.DataLoader(train_set, batch_size=NUMBER_OF_ELEMENTS, shuffle=True)\n",
    "\n",
    "# Random image\n",
    "dataiter = iter(train_loader_vis)\n",
    "images, labels = next(dataiter)\n",
    "\n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "print('Labels :', ' '.join(f'{labels[j].item()};' for j in range(NUMBER_OF_ELEMENTS)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also look at different attributes like the number of images in the dataset, the size of each image or the label of an image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image : tensor([[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0118, 0.0706, 0.0706, 0.0706,\n",
      "          0.4941, 0.5333, 0.6863, 0.1020, 0.6510, 1.0000, 0.9686, 0.4980,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.1176, 0.1412, 0.3686, 0.6039, 0.6667, 0.9922, 0.9922, 0.9922,\n",
      "          0.9922, 0.9922, 0.8824, 0.6745, 0.9922, 0.9490, 0.7647, 0.2510,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1922,\n",
      "          0.9333, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922,\n",
      "          0.9922, 0.9843, 0.3647, 0.3216, 0.3216, 0.2196, 0.1529, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0706,\n",
      "          0.8588, 0.9922, 0.9922, 0.9922, 0.9922, 0.9922, 0.7765, 0.7137,\n",
      "          0.9686, 0.9451, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.3137, 0.6118, 0.4196, 0.9922, 0.9922, 0.8039, 0.0431, 0.0000,\n",
      "          0.1686, 0.6039, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0549, 0.0039, 0.6039, 0.9922, 0.3529, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.5451, 0.9922, 0.7451, 0.0078, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0431, 0.7451, 0.9922, 0.2745, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.1373, 0.9451, 0.8824, 0.6275,\n",
      "          0.4235, 0.0039, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3176, 0.9412, 0.9922,\n",
      "          0.9922, 0.4667, 0.0980, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1765, 0.7294,\n",
      "          0.9922, 0.9922, 0.5882, 0.1059, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0627,\n",
      "          0.3647, 0.9882, 0.9922, 0.7333, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.9765, 0.9922, 0.9765, 0.2510, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1804, 0.5098,\n",
      "          0.7176, 0.9922, 0.9922, 0.8118, 0.0078, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.1529, 0.5804, 0.8980, 0.9922,\n",
      "          0.9922, 0.9922, 0.9804, 0.7137, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0941, 0.4471, 0.8667, 0.9922, 0.9922, 0.9922,\n",
      "          0.9922, 0.7882, 0.3059, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0902, 0.2588, 0.8353, 0.9922, 0.9922, 0.9922, 0.9922, 0.7765,\n",
      "          0.3176, 0.0078, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0706, 0.6706,\n",
      "          0.8588, 0.9922, 0.9922, 0.9922, 0.9922, 0.7647, 0.3137, 0.0353,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.2157, 0.6745, 0.8863, 0.9922,\n",
      "          0.9922, 0.9922, 0.9922, 0.9569, 0.5216, 0.0431, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.5333, 0.9922, 0.9922, 0.9922,\n",
      "          0.8314, 0.5294, 0.5176, 0.0627, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000],\n",
      "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "          0.0000, 0.0000, 0.0000, 0.0000]]])\n",
      "------------------------------------------------------------\n",
      "image shape : torch.Size([1, 28, 28])\n",
      "label : 5\n"
     ]
    }
   ],
   "source": [
    "image, label = train_set[0]\n",
    "\n",
    "print(\"image :\", image) # pixels value if you want to see the matrix\n",
    "print(\"-\"*60)\n",
    "print(\"image shape :\", image.shape) # pixels value\n",
    "print(\"label :\", label) # Number represented in the image\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, we have images **28 pixels high and 28 pixels wide**, with **one channel** (grayscale !).\n",
    "\n",
    "These images represent a number from 0 to 9, we have **10 different labels** (or 10 different possible output).\\\n",
    "The first picture represents a 5, therefore its label is 5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Batch-Size\n",
    "\n",
    "Did you remember when we talk about batch and parallelization of multiple example with torch ? This is very important here !\n",
    "\n",
    "**60,000** is a lot of images to process one by one, to make it easier for our model to process this data while training we are going to use ``batch_size``.\n",
    "\n",
    "for one who forget , ``batch_size`` is a hyperparameter that defines the number of samples to work through before updating the internal model parameters. In other words, before calculating the error and apply backpropagation after each image, if our batch size is 64 we will go through 64 images before doing it. **This improves the learning of our AI** by **applying the backpropagation on the error average.**\n",
    "\n",
    "As in the previous notebook we will use a [**``dataloader``**](https://pytorch.org/docs/stable/data.html), this time we don't need to redefine a ``Dataset`` class since we are using a ``builtin`` dataset in ``torchvision``.\n",
    "\n",
    "Remember to specify that you use the ``train_set`` and you want a ``batch_size`` of ``64`` and also ``shuffle`` it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image shape : torch.Size([64, 1, 28, 28])\n",
      "labels shape : torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "#TODO : Define the batch size\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_set, BATCH_SIZE, True)\n",
    "\n",
    "assert len(train_loader) == 938, \"Your train loader is not well implemented, remember that the batch size is 64\"\n",
    "\n",
    "batch = next(iter(train_loader)) # obtain the first batch\n",
    "images, labels = batch\n",
    "print(\"image shape :\", images.shape)\n",
    "print(\"labels shape :\", labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have `938` lots containing `64` images each (and their equivalent labels).\\\n",
    "This will **drastically decrease our training time** because with one backward propagation, 64 images are processed.\n",
    "\n",
    "\n",
    "> Pytorch is built to be used with batch, it is thus quite simple to implement it in our code. \n",
    "\n",
    "*you can try after to change your batch and see the difference in the learning (remove the assert for test it)* !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Also load the test set with the same batch_size...\n",
    "\n",
    "eval_loader = torch.utils.data.DataLoader(eval_set, BATCH_SIZE, True)\n",
    "\n",
    "assert len(eval_loader) == 157, \"Your eval loader is not well implemented\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Model!\n",
    "\n",
    "And your moment has arrived!\n",
    "\n",
    "I’m sure you’ve been eagerly anticipating this step, and now you’re ready to build your very first real neural network, complete with a more complex architecture.\n",
    "\n",
    "A quick tip for working with PyTorch: today’s task is a classification problem, as we’ve defined specific output labels. For this, we’ll be using the **[cross-entropy](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html)** loss function. (Remember, yesterday you used the **[binary cross-entropy](https://pytorch.org/docs/stable/generated/torch.nn.BCELoss.html)** loss with logistic regression, since the output was restricted to just 0 or 1.)\n",
    "\n",
    "*Don't hesistate to jump at the end of the torch introduction as helping you for initialize the model and train it !*\n",
    "\n",
    "IF you encounter difficulties to create your model, at the end of this notebook there is a pseudo code of the architecture as to help you to create the model, but try to do it alone ! (with everything you see before)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO : Define the learning rate\n",
    "LEARNING_RATE = 0.01\n",
    "\n",
    "\n",
    "class MNISTModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MNISTModel, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(1, 1)\n",
    "        #TODO : add other layers if you want\n",
    "        self.c1_2d = nn.Conv2d(1, 1, 3)\n",
    "\n",
    "        self.loss = nn.CrossEntropyLoss() # Loss function cross entropy\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=LEARNING_RATE) # Optimizer Adam\n",
    "        #TODO : add other optimizers if you want\n",
    "        self.relu = nn.ReLU() # Activation function\n",
    "        self.softmax= nn.Softmax(dim = 1)\n",
    "\n",
    "        # Device choice\n",
    "        if torch.cuda.is_available():\n",
    "            self.device = torch.device('cuda')\n",
    "        elif torch.backends.mps.is_available():\n",
    "            self.device = torch.device('mps')\n",
    "        else:\n",
    "            self.device = torch.device('cpu')\n",
    "        print(f\"Device : {self.device}\")\n",
    "        self.to(self.device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #TODO : Define the forward pass\n",
    "        x = self.c1_2d(x)\n",
    "        self.flatten(x)\n",
    "        return self.fc1(x)\n",
    "\n",
    "\n",
    "    def train_model(self, epochs, train_loader):\n",
    "        self.train()  # Training mode\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            start_time = time.time()  # Start time of the epoch\n",
    "            running_loss = 0.0\n",
    "            total_batches = 0\n",
    "\n",
    "            for i, data in enumerate(train_loader): # Enumerate the data, all the dataset\n",
    "                inputs, labels = data\n",
    "                inputs, labels = inputs.to(self.device), labels.to(self.device)\n",
    "\n",
    "                #TODO Compute the training part ~ 5 lines\n",
    "                ...\n",
    "                ###################################\n",
    "\n",
    "                # Gradient to zero\n",
    "                ...\n",
    "\n",
    "                # Forward pass\n",
    "                outputs = self.forward(data)\n",
    "\n",
    "                # Loss calculation\n",
    "                loss = self.loss(inputs, outputs)\n",
    "\n",
    "                # Backward pass\n",
    "                ...\n",
    "\n",
    "                # Optimisation step\n",
    "                ...\n",
    "\n",
    "                running_loss += loss.item()\n",
    "                total_batches += 1 # just help for print \n",
    "\n",
    "                # print every 8 mini-batches\n",
    "                if (i + 1) % 8 == 0 or (i + 1) == len(train_loader):\n",
    "                    print(f\"\\rEpochs {epoch + 1}/{epochs} | Lot {i + 1}/{len(train_loader)} | Loss : {loss.item():.4f}\", end='')\n",
    "\n",
    "            \n",
    "            avg_loss = running_loss / len(train_loader)\n",
    "            epoch_time = time.time() - start_time\n",
    "\n",
    "            print(\"\\n\")\n",
    "            print(\"-\" * 60)\n",
    "            print(f\"Epochs {epoch + 1}/{epochs} finish | Average Loss : {avg_loss:.4f} | Time : {epoch_time:.2f} seconds\")\n",
    "            print(\"-\" * 60)\n",
    "\n",
    "        # change the model_path if you want\n",
    "        model_path = \"mnist_model.pth\"\n",
    "        print('Training finished, saving model to :', model_path)\n",
    "        torch.save(self.state_dict(), model_path)\n",
    "\n",
    "\n",
    "    def eval_model(self, test_loader):\n",
    "        self.eval()  # Evaluation mode\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for data in test_loader:\n",
    "                images, labels = data\n",
    "                images, labels = images.to(self.device), labels.to(self.device)\n",
    "                outputs = self(images)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "        print(f'Accuracy of the model on {total} images is : {100 * correct / total:.2f}%')\n",
    "\n",
    "    def load_weights(self, model_path):\n",
    "        self.load_state_dict(torch.load(model_path, weights_only=True, map_location=self.device))\n",
    "        self.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well done ! you need now to initialise your model by simple call your python class, \n",
    "\n",
    "It permits that if you want to restart the training with random weights, you can restart this cell. Otherwise, the training if (you restart it) will continue from the **`last loss value`** and the **`last weight`**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device : cpu\n"
     ]
    }
   ],
   "source": [
    "my_model = MNISTModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "relu(): argument 'input' (position 1) must be Tensor, not list",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[48]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m#TODO: define number of epochs\u001b[39;00m\n\u001b[32m      2\u001b[39m EPOCHS = \u001b[32m10000\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[43mmy_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mEPOCHS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[46]\u001b[39m\u001b[32m, line 53\u001b[39m, in \u001b[36mMNISTModel.train_model\u001b[39m\u001b[34m(self, epochs, train_loader)\u001b[39m\n\u001b[32m     49\u001b[39m ...\n\u001b[32m     50\u001b[39m \u001b[38;5;66;03m###################################\u001b[39;00m\n\u001b[32m     51\u001b[39m \n\u001b[32m     52\u001b[39m \u001b[38;5;66;03m# Gradient to zero\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m53\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrelu\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     55\u001b[39m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[32m     56\u001b[39m outputs = \u001b[38;5;28mself\u001b[39m.forward(data)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/delivery/Tek1/Pool/Poc_Pool/day_04/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1776\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1774\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1775\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1776\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/delivery/Tek1/Pool/Poc_Pool/day_04/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1787\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1784\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1786\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1787\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1789\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1790\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/delivery/Tek1/Pool/Poc_Pool/day_04/.venv/lib/python3.12/site-packages/torch/nn/modules/activation.py:143\u001b[39m, in \u001b[36mReLU.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    139\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m    140\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    141\u001b[39m \u001b[33;03m    Runs the forward pass.\u001b[39;00m\n\u001b[32m    142\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m143\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrelu\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minplace\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/delivery/Tek1/Pool/Poc_Pool/day_04/.venv/lib/python3.12/site-packages/torch/nn/functional.py:1721\u001b[39m, in \u001b[36mrelu\u001b[39m\u001b[34m(input, inplace)\u001b[39m\n\u001b[32m   1719\u001b[39m     result = torch.relu_(\u001b[38;5;28minput\u001b[39m)\n\u001b[32m   1720\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1721\u001b[39m     result = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrelu\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   1722\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[31mTypeError\u001b[39m: relu(): argument 'input' (position 1) must be Tensor, not list"
     ]
    }
   ],
   "source": [
    "#TODO: define number of epochs\n",
    "EPOCHS = 10000\n",
    "\n",
    "my_model.train_model(EPOCHS, train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "you can now test your model by simply call the eval function !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_model.eval_model(eval_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you’d like to retrain and check for better results, simply re-run the training cell or initialize a new model to start fresh!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Play with your model !\n",
    "\n",
    "Now it's time to test your own model! **Please paste your model architecture** (*`__init__`* and *`forward`* methods) into the file [model.py](model.py), and run the following command in the terminal:\n",
    "\n",
    "```bash\n",
    "python app.py\n",
    "```\n",
    "after this break, you have two option : \n",
    "\n",
    "- ***2.2 - Cifar*** -> try to implemente an really complex architecture called VAE-GAN for another task \n",
    "\n",
    "- ***3.1 - My torch*** -> try to recreate some function of torch, to really understand how this is work (it my be help you for creating a VAE-GAN architecture :))\n",
    "\n",
    "choose one ! *(you can do both also if you finish in advance)*\n",
    "\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO : Define the learning rate\n",
    "LEARNING_RATE = ...\n",
    "\n",
    "\n",
    "class MNISTModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MNISTModel, self).__init__()\n",
    "        self.flatten = ... # Flatten the data\n",
    "        self.fc1 = ... # Fully connected layer from 28**28 to 128\n",
    "        self.fc2 = ... # Fully connected layer from 128 to 64\n",
    "        self.fc3 = ... # Fully connected layer from 64 to 10\n",
    "\n",
    "        self.loss = ... # Loss function cross entropy\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=LEARNING_RATE) # Optimizer Adam\n",
    "        self.relu = ... # Activation function\n",
    "        \n",
    "        # Device choice \n",
    "        if torch.cuda.is_available():\n",
    "            self.device = torch.device('cuda')\n",
    "        elif torch.backends.mps.is_available():\n",
    "            self.device = torch.device('mps')\n",
    "        else:\n",
    "            self.device = torch.device('cpu')\n",
    "        print(f\"Device : {self.device}\")\n",
    "        self.to(self.device)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = ... # Flatten the data\n",
    "\n",
    "        ... # Compute your self.fc1\n",
    "        ... # Activation function\n",
    "\n",
    "        ... # Compute your self.fc2\n",
    "        ... # Activation function\n",
    "        ... # Compute your self.fc3\n",
    "\n",
    "        return ...\n",
    "\n",
    "\n",
    "    def train_model(self, epochs, train_loader):\n",
    "        self.train()  # Training mode\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            start_time = time.time()  # Start time of the epoch\n",
    "            running_loss = 0.0\n",
    "            total_batches = 0\n",
    "\n",
    "            for i, data in enumerate(train_loader): # Enumerate the data, all the dataset\n",
    "                inputs, labels = data\n",
    "                inputs, labels = inputs.to(self.device), labels.to(self.device)\n",
    "                \n",
    "                # Gradient to zero\n",
    "                ...\n",
    "\n",
    "                # Forward pass\n",
    "                outputs = ...\n",
    "\n",
    "                # Loss calculation\n",
    "                loss = ...\n",
    "\n",
    "                # Backward pass\n",
    "                ...\n",
    "\n",
    "                # Optimisation step\n",
    "                ...\n",
    "\n",
    "                running_loss += loss.item()\n",
    "                total_batches += 1 # just help for print \n",
    "\n",
    "                # print every 8 mini-batches\n",
    "                if (i + 1) % 8 == 0 or (i + 1) == len(train_loader):\n",
    "                    print(f\"\\rEpochs {epoch + 1}/{epochs} | Lot {i + 1}/{len(train_loader)} | Loss : {loss.item():.4f}\", end='')\n",
    "\n",
    "            \n",
    "            avg_loss = running_loss / len(train_loader)\n",
    "            epoch_time = time.time() - start_time\n",
    "\n",
    "            print(\"\\n\")\n",
    "            print(\"-\" * 60)\n",
    "            print(f\"Epochs {epoch + 1}/{epochs} finish | Average Loss : {avg_loss:.4f} | Time : {epoch_time:.2f} seconds\")\n",
    "            print(\"-\" * 60)\n",
    "\n",
    "        # change the model_path if you want\n",
    "        model_path = \"mnist_model.pth\"\n",
    "        print('Training finished, saving model to :', model_path)\n",
    "        torch.save(self.state_dict(), model_path)\n",
    "\n",
    "\n",
    "    def eval_model(self, test_loader):\n",
    "        self.eval()  # Evaluation mode\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for data in test_loader:\n",
    "                images, labels = data\n",
    "                images, labels = images.to(self.device), labels.to(self.device)\n",
    "                outputs = self(images)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "        print(f'Accuracy of the model on {total} images is : {100 * correct / total:.2f}%')\n",
    "\n",
    "    def load_weights(self, model_path):\n",
    "        self.load_state_dict(torch.load(model_path, weights_only=True, map_location=self.device))\n",
    "        self.eval()\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
